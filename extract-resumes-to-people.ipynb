{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:18:47.318851Z",
     "start_time": "2025-06-01T14:18:47.262630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def read_resumes_from_directory(directory=\"resume-pdfs\"):\n",
    "    \"\"\"\n",
    "    Read all PDFs from a directory and return a list of text strings\n",
    "    \"\"\"\n",
    "    resumes = []\n",
    "\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return resumes\n",
    "\n",
    "    # Get all PDF files from the directory\n",
    "    pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{directory}'.\")\n",
    "        return resumes\n",
    "\n",
    "    # Process each PDF file\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(directory, pdf_file)\n",
    "\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            text = \"\"\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "\n",
    "            resumes.append(text)\n",
    "            print(f\"Processed: {pdf_file} ({len(text)} characters)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {pdf_file}: {str(e)}\")\n",
    "\n",
    "    print(f\"Total resumes extracted: {len(resumes)}\")\n",
    "    return resumes\n",
    "\n",
    "\n",
    "# Call the function to get the list of resume texts\n",
    "resumes = read_resumes_from_directory()\n",
    "resumes[:1]"
   ],
   "id": "961d910f6e785076",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: resume_9_amanda_foster.pdf (1878 characters)\n",
      "Processed: resume_1_sarah_chen.pdf (1485 characters)\n",
      "Processed: resume_8_monica_garcia.pdf (1702 characters)\n",
      "Processed: resume_7_robert_johnson.pdf (1682 characters)\n",
      "Processed: resume_5_david_kim.pdf (1548 characters)\n",
      "Processed: resume_3_jennifer_park.pdf (1575 characters)\n",
      "Processed: resume_4_alex_thompson.pdf (1439 characters)\n",
      "Processed: resume_10_james_mitchell.pdf (2065 characters)\n",
      "Processed: resume_2_marcus_rodriguez.pdf (1342 characters)\n",
      "Processed: resume_6_lisa_wang.pdf (1509 characters)\n",
      "Total resumes extracted: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dr. Amanda Foster\\nResearch Scientist\\nEmail: amanda.foster@email.com\\nLocation: Cambridge, MA\\nExperience: 8 years\\nProfessional Summary\\nAI research scientist with 8 years experience in computer vision and natural language processing. PhD\\nin Computer Science with focus on deep learning applications.\\nProfessional Experience\\nPrincipal Research Scientist | AI Research Institute | 2021 - Present\\n- Published 15 peer-reviewed papers on computer vision and NLP, cited 500+ times in academic\\nliterature\\n- Built state-of-the-art image recognition model achieving top-3 accuracy on ImageNet benchmark\\n- Led research team of 6 PhD students and postdocs investigating multimodal AI systems\\nResearch Scientist | Tech Giant Research Lab | 2018 - 2021\\n- Developed novel neural network architecture for language understanding, open-sourced for research\\ncommunity\\n- Shipped research prototype to production, serving millions of users with real-time image analysis\\n- Collaborated with engineering teams to transfer research innovations into commercial products\\nPostdoctoral Researcher | MIT CSAIL | 2016 - 2018\\n- Conducted fundamental research in deep learning and computer vision\\n- Won \"Best Paper Award\" at Computer Vision and Pattern Recognition (CVPR) conference\\nTechnical Skills\\n- AI/ML: 10+ years Python, 8 years Machine Learning, 6 years Deep Learning, 5 years Computer\\nVision, 4 years Natural Language Processing\\n- Frameworks: 3 years TensorFlow, 2 years PyTorch\\n- Research: 8 years Research methodology and publication\\nSelected Publications\\n1. \"Multimodal Attention Networks for Visual Question Answering\" - CVPR 2023\\n2. \"Self-Supervised Learning for Image Classification\" - NeurIPS 2022\\n3. \"Transformer Architectures for Computer Vision\" - ICLR 2021\\nEducation\\nPh.D. Computer Science | MIT | 2016\\nM.S. Computer Science | Stanford University | 2012\\nB.S. Computer Science | Caltech | 2010\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T14:09:43.173559Z",
     "start_time": "2025-06-01T14:09:43.166069Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "env_file = '.env'\n",
    "load_dotenv('.env', override=True)\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI AI API key: \")"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1489116658513747"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ee6e614d19aa12a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "45332be536d5507c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "42d71cb36a4a7d4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:28:13.838495Z",
     "start_time": "2025-06-01T14:28:13.820855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "from typing import List\n",
    "from tqdm.asyncio import tqdm as tqdm_async\n",
    "import asyncio\n",
    "\n",
    "def chunks(xs, n=10):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]\n",
    "\n",
    "\n",
    "\n",
    "class TextExtractor:\n",
    "    def __init__(self,\n",
    "                 llm_with_struct_output,\n",
    "                 prompt_template: PromptTemplate):\n",
    "        self.llm = llm_with_struct_output\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    async def extract(self, texts: List[str], semaphore) -> BaseModel:\n",
    "        async with semaphore:\n",
    "            prompt = self.prompt_template.invoke({'texts': '\\n\\n'.join(texts)})\n",
    "            # Use structured LLM for extraction\n",
    "            entity: BaseModel = await self.llm.ainvoke(prompt)\n",
    "        return entity\n",
    "\n",
    "\n",
    "    async def extract_all(self, texts: List[str], chunk_size=1, max_workers=10) -> List[BaseModel]:\n",
    "        # Create a semaphore with the desired number of workers\n",
    "        semaphore = asyncio.Semaphore(max_workers)\n",
    "\n",
    "        # Create tasks with the semaphore\n",
    "        text_chunks = chunks(texts, chunk_size)\n",
    "        tasks = [self.extract(text_chunk, semaphore) for text_chunk in text_chunks]\n",
    "\n",
    "        # Explicitly update progress using `tqdm` as tasks complete\n",
    "        entities: List[BaseModel] = []\n",
    "        with tqdm_async(total=len(tasks), desc=\"extracting texts\") as pbar:\n",
    "            for future in asyncio.as_completed(tasks):\n",
    "                result = await future\n",
    "                entities.append(result)\n",
    "                pbar.update(1)  # Increment progress bar for each completed task\n",
    "        return entities"
   ],
   "id": "e7d513b47b430f30",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:28:16.891590Z",
     "start_time": "2025-06-01T14:28:16.888988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "people_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are extracting information from resumes according to the people schema. Below is the resume\n",
    "# Resumes\n",
    "{texts}\n",
    "\"\"\")"
   ],
   "id": "e57f0b353ee9d9bb",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:28:21.277782Z",
     "start_time": "2025-06-01T14:28:21.233236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from person import Person\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0).with_structured_output(Person)"
   ],
   "id": "69cd76b9e20af751",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:28:22.379732Z",
     "start_time": "2025-06-01T14:28:22.377631Z"
    }
   },
   "cell_type": "code",
   "source": "text_extractor = TextExtractor(llm, people_prompt_template)",
   "id": "5b43cb9dbcb2734d",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:28:41.564130Z",
     "start_time": "2025-06-01T14:28:23.482944Z"
    }
   },
   "cell_type": "code",
   "source": "people = await text_extractor.extract_all(resumes)",
   "id": "8fab7e1fa2fbf892",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting texts: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:28:41.590611Z",
     "start_time": "2025-06-01T14:28:41.586688Z"
    }
   },
   "cell_type": "code",
   "source": "type(people)",
   "id": "74702b6489a3c98b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:28:41.671919Z",
     "start_time": "2025-06-01T14:28:41.669694Z"
    }
   },
   "cell_type": "code",
   "source": "len(people)",
   "id": "9ee564b7bfba5e71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:32:07.227401Z",
     "start_time": "2025-06-01T14:32:07.210360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "people_list = [person.model_dump() for person in people]\n",
    "\n",
    "with open('extracted-people-data.json', 'w') as json_file:\n",
    "    json.dump(people_list, json_file, indent=4)\n",
    "\n",
    "print(\"JSON file created: people_data.json\")\n"
   ],
   "id": "c4f3ef2d9a1f34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created: people_data.json\n"
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
